import numpy as np
from sklearn.preprocessing import StandardScaler

file_path = "data.txt"  

data = np.loadtxt(file_path)

X = data[:, :-1]  
y = data[:, -1]    


scaler = StandardScaler()
X = scaler.fit_transform(X)  
print("Valeurs moyennes après normalisation :", np.mean(X, axis=0))
print("Écart-type après normalisation :", np.std(X, axis=0))
class MLP:
    def __init__(self, layer_sizes, activation='relu',learning_rate=0.01):
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.activation_func = self.relu if activation == 'relu' else self.sigmoid
        self.activation_derivative = self.relu_derivative if activation == 'relu' else self.sigmoid_derivative


self.weights = [np.random.randn(layer_sizes[i + 1], layer_sizes[i]) * np.sqrt(2 / layer_sizes[i]) 
                        for i in range(len(layer_sizes) - 1)]
        self.biases = [np.zeros((layer_sizes[i + 1], 1)) for i in range(len(layer_sizes) - 1)]

 def relu(self, Z):
        return np.maximum(0, Z)
def relu_derivative(self, Z):
        return (Z > 0).astype(float)

def sigmoid(self, Z):
        return 1 / (1 + np.exp(-Z))
def sigmoid_derivative(self, Z):
        return self.sigmoid(Z) * (1 - self.sigmoid(Z))

    def forward_propagation(self, X):
        A = X.T  
        activations = [A]  
        zs = [] 
 for i in range(len(self.weights)):
            Z = np.dot(self.weights[i], A) + self.biases[i]
            if i == len(self.weights) - 1:  
                A = self.sigmoid(Z)  
            else:
                A = self.activation_func(Z)
            activations.append(A)

        return A, activations, zs
#RÉTROPROPAGATION & ENTRAÎNEMENT
  def backward_propagation(self, X, y, activations, zs):
        m = y.shape[0]
        grads_w = [np.zeros(w.shape) for w in self.weights]
        grads_b = [np.zeros(b.shape) for b in self.biases]
# Calcul de l'erreur en sortie 
        A_final = activations[-1]
        delta = A_final - y.T  # Erreur (dernière couche)
        grads_w[-1] = np.dot(delta, activations[-2].T) / m
        grads_b[-1] = np.mean(delta, axis=1, keepdims=True)

# ======== TEST DU MLP ========
if __name__ == "__main__":
    mlp = MLP([3, 5, 3, 1], activation='relu')

    output, activations = mlp.forward_propagation(X)

print("Min:", np.min(output))
print("Max:", np.max(output))
print("Mean:", np.mean(output))
print("Sortie du réseau (premiers 10 résultats) :")
print(output[:, :10])  
