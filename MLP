import numpy as np
from sklearn.preprocessing import StandardScaler

file_path = "data.txt"  

data = np.loadtxt(file_path)

X = data[:, :-1]  
y = data[:, -1]    


scaler = StandardScaler()
X = scaler.fit_transform(X)  
print("Valeurs moyennes après normalisation :", np.mean(X, axis=0))
print("Écart-type après normalisation :", np.std(X, axis=0))
class MLP:
    def __init__(self, layer_sizes, activation='relu',learning_rate=0.01):
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.activation_func = self.relu if activation == 'relu' else self.sigmoid
        self.activation_derivative = self.relu_derivative if activation == 'relu' else self.sigmoid_derivative


self.weights = [np.random.randn(layer_sizes[i + 1], layer_sizes[i]) * np.sqrt(2 / layer_sizes[i]) 
                        for i in range(len(layer_sizes) - 1)]
        self.biases = [np.zeros((layer_sizes[i + 1], 1)) for i in range(len(layer_sizes) - 1)]

 def relu(self, Z):
        return np.maximum(0, Z)
def relu_derivative(self, Z):
        return (Z > 0).astype(float)

def sigmoid(self, Z):
        return 1 / (1 + np.exp(-Z))
def sigmoid_derivative(self, Z):
        return self.sigmoid(Z) * (1 - self.sigmoid(Z))

    def forward_propagation(self, X):
        A = X.T  
        activations = [A]  
        zs = [] 
 for i in range(len(self.weights)):
            Z = np.dot(self.weights[i], A) + self.biases[i]
            if i == len(self.weights) - 1:  
                A = self.sigmoid(Z)  
            else:
                A = self.activation_func(Z)
            activations.append(A)

        return A, activations, zs
#RÉTROPROPAGATION & ENTRAÎNEMENT
  def backward_propagation(self, X, y, activations, zs):
        m = y.shape[0]
        grads_w = [np.zeros(w.shape) for w in self.weights]
        grads_b = [np.zeros(b.shape) for b in self.biases]
# Calcul de l'erreur en sortie 
        A_final = activations[-1]
        delta = A_final - y.T  # Erreur (dernière couche)
        grads_w[-1] = np.dot(delta, activations[-2].T) / m
        grads_b[-1] = np.mean(delta, axis=1, keepdims=True)

# Rétropropagation sur les couches cachées
        for l in range(len(self.weights) - 2, -1, -1):
            delta = np.dot(self.weights[l + 1].T, delta) * self.activation_derivative(zs[l])
            grads_w[l] = np.dot(delta, activations[l].T) / m
            grads_b[l] = np.mean(delta, axis=1, keepdims=True)

        return grads_w, grads_b

def update_parameters(self, grads_w, grads_b):
        for i in range(len(self.weights)):
            self.weights[i] -= self.learning_rate * grads_w[i]
            self.biases[i] -= self.learning_rate * grads_b[i]

    def train(self, X, y, epochs=1000):
        for epoch in range(epochs):
            output, activations, zs = self.forward_propagation(X)
            grads_w, grads_b = self.backward_propagation(X, y, activations, zs)
            self.update_parameters(grads_w, grads_b)

            if epoch % 100 == 0:
                loss = np.mean((output - y.T) ** 2)
                print(f"Epoch {epoch}: Loss = {loss:.5f}")


# ======== TEST DU MLP ========
if __name__ == "__main__":
    mlp = MLP([3, 5, 3, 1], activation='relu', learning_rate=0.01)

 # Entraînement du réseau
    mlp.train(X, y, epochs=1000)

    output, activations = mlp.forward_propagation(X)

print("Min:", np.min(output))
print("Max:", np.max(output))
print("Mean:", np.mean(output))
print("Sortie du réseau (premiers 10 résultats) :")
print(output[:, :10])  

if __name__ == "__main__":
    #Définition des données XOR
    X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y_xor = np.array([[0], [1], [1], [0]])  

    #Création et entraînement du MLP
    mlp_xor = MLP([2, 5, 1], activation='relu', learning_rate=0.1)

    mlp_xor.train(X_xor, y_xor, epochs=5000)

    #Test du réseau sur XOR
    output_xor, _, _ = mlp_xor.forward_propagation(X_xor)

    print("\nRésultats du test XOR :")
    print("Entrées :\n", X_xor)
    print("Sorties attendues :\n", y_xor.T)
    print("Sorties du MLP :\n", np.round(output_xor))
