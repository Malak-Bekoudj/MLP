import numpy as np
from sklearn.preprocessing import StandardScaler

file_path = "data.txt"  

data = np.loadtxt(file_path)

X = data[:, :-1]  
y = data[:, -1]    


scaler = StandardScaler()
X = scaler.fit_transform(X)  
print("Valeurs moyennes après normalisation :", np.mean(X, axis=0))
print("Écart-type après normalisation :", np.std(X, axis=0))
class MLP:
    def __init__(self, layer_sizes, activation='relu'):
        self.layer_sizes = layer_sizes
        self.activation_func = self.relu if activation == 'relu' else self.sigmoid

self.weights = [np.random.randn(layer_sizes[i + 1], layer_sizes[i]) * np.sqrt(2 / layer_sizes[i]) 
                        for i in range(len(layer_sizes) - 1)]
        self.biases = [np.zeros((layer_sizes[i + 1], 1)) for i in range(len(layer_sizes) - 1)]

 def relu(self, Z):
        return np.maximum(0, Z)

    def sigmoid(self, Z):
        return 1 / (1 + np.exp(-Z))

    def forward_propagation(self, X):
        A = X.T  
        activations = [A]  
 for i in range(len(self.weights)):
            Z = np.dot(self.weights[i], A) + self.biases[i]
            if i == len(self.weights) - 1:  
                A = self.sigmoid(Z)  
            else:
                A = self.activation_func(Z)
            activations.append(A)

        return A, activations

# ======== TEST DU MLP ========
if __name__ == "__main__":
    mlp = MLP([3, 5, 3, 1], activation='relu')

    output, activations = mlp.forward_propagation(X)

print("Min:", np.min(output))
print("Max:", np.max(output))
print("Mean:", np.mean(output))
print("Sortie du réseau (premiers 10 résultats) :")
print(output[:, :10])  
